{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *This notebook is just for showing all the steps of the data processing for GCN, but for saving time all the output data of this notebook have already been uploaded to the cloud and we don't need to run it during the educational session.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Haxby dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are fetching the data for subject 4\n",
    "data_dir = os.path.join('..', 'data')\n",
    "sub_no = 4\n",
    "haxby_ds = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)\n",
    "\n",
    "func_file = haxby_ds.func[0]\n",
    "\n",
    "# Standardizing\n",
    "mask_vt_file = haxby_ds.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_vt_file, standardize=True)\n",
    "\n",
    "labels = pd.read_csv(haxby_ds.session_target[0], sep=\" \")\n",
    "\n",
    "# Selecting data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = labels['labels']\n",
    "\n",
    "categories = y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = os.path.join(data_dir, 'haxby_proc/')\n",
    "concat_path = os.path.join(data_dir, 'haxby_concat/')\n",
    "conn_path = os.path.join(data_dir, 'haxby_connectomes/')\n",
    "split_path = os.path.join(data_dir, 'haxby_split_win/')\n",
    "\n",
    "if not os.path.exists(proc_path):\n",
    "    os.makedirs(proc_path)\n",
    "if not os.path.exists(concat_path):\n",
    "    os.makedirs(concat_path)\n",
    "if not os.path.exists(conn_path):\n",
    "    os.makedirs(conn_path)\n",
    "if not os.path.exists(split_path):\n",
    "    os.makedirs(split_path)\n",
    "    \n",
    "# delete the contents of a folder to avoid inconsistency\n",
    "old_files = glob.glob(concat_path + '/*')\n",
    "for f in old_files:\n",
    "    os.remove(f)    \n",
    "if os.path.exists(split_path):\n",
    "    files = glob.glob(os.path.join(split_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "old_dirContents = os.listdir(concat_path)\n",
    "print(old_dirContents)\n",
    "\n",
    "concat_bold_files = []\n",
    "if (len(old_dirContents) == 0 or len(old_dirContents) == 1):    \n",
    "    if ((len(X)) == len(y)):\n",
    "        \n",
    "        for i in range(0,len(y)):\n",
    "            label = y[i]\n",
    "            concat_bold_files = X[i:i+1]\n",
    "            concat_file_name = concat_path + '{}_concat_fMRI.npy'.format(label)\n",
    "            file = pathlib.Path(concat_file_name)\n",
    "            \n",
    "            if file.exists ():\n",
    "                concat_file = np.load(concat_file_name, allow_pickle = True)\n",
    "                concat_file = np.concatenate((concat_file, concat_bold_files), axis = 0)\n",
    "                np.save(concat_file_name, concat_file)\n",
    "            else:\n",
    "                np.save(concat_file_name, concat_bold_files)\n",
    "            \n",
    "else:\n",
    "    print('Folder is Not Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(concat_path + 'phenotypic_data.tsv', 'wt') as out_file:\n",
    "    \n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow(['label'])\n",
    "    \n",
    "    for category in categories: \n",
    "        tsv_writer.writerow([category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 1\n",
    "\n",
    "# Path for saving the files\n",
    "pheno_file = os.path.join(concat_path, 'phenotypic_data.tsv')\n",
    "processed_bold_files = sorted(glob.glob(concat_path + '/*.npy'))\n",
    "out_file = os.path.join(split_path, '{}_{:04d}.npy')\n",
    "out_csv = os.path.join(split_path, 'labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split fMRI data\n",
    "Now we are going to split bold input files to the desired windows lenght, then we will also create a csv file that will contain label for each splited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_labels = {'rest':0,'face':1,'chair':2,'scissors':3,'shoe':4,'scrambledpix':5,'house':6,'cat':7,'bottle':8}\n",
    "label_df = pd.DataFrame(columns=['label', 'filename'])\n",
    "\n",
    "for proc_bold in processed_bold_files:\n",
    "    \n",
    "    ts_data = np.load(proc_bold)\n",
    "    ts_duration = len(ts_data)\n",
    "\n",
    "    ts_filename = os.path.basename(proc_bold)\n",
    "    ts_label = ts_filename.split('_', 1)[0]\n",
    "\n",
    "    valid_label = dic_labels[ts_label]\n",
    "    \n",
    "    # Split the timeseries\n",
    "    rem = ts_duration % window_length\n",
    "    n_splits = int(np.floor(ts_duration / window_length))\n",
    "\n",
    "    ts_data = ts_data[:(ts_duration-rem), :]   \n",
    "    \n",
    "    for j, split_ts in enumerate(np.split(ts_data, n_splits)):\n",
    "        ts_output_file_name = out_file.format(ts_filename, j)\n",
    "\n",
    "        split_ts = np.swapaxes(split_ts, 0, 1)\n",
    "        np.save(ts_output_file_name, split_ts)\n",
    "        curr_label = {'label': valid_label, 'filename': os.path.basename(ts_output_file_name)}\n",
    "        label_df = label_df.append(curr_label, ignore_index=True)\n",
    "    \n",
    "label_df.to_csv(out_csv, index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
